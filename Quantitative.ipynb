{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "%pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "%pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Parts and Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import tokenizer_utils\n",
    "def do_nothing(*args, **kwargs):\n",
    "    pass\n",
    "tokenizer_utils.fix_untrained_tokens = do_nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Version: 8, Minor Version: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.533 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from typing import Tuple, Any, Dict, List, Union\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Models that have been tested\n",
    "used_models = [\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",   \n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/gemma-2-9b-it-bnb-4bit\",    \n",
    "    \"unsloth/Qwen2-1.5B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/Qwen2-7B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",     \n",
    "    \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",   \n",
    "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\", \n",
    "    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\", \n",
    "    \"unsloth/Phi-3-medium-4k-instruct-bnb-4bit\",     \n",
    "]\n",
    "\n",
    "#Check CUDA device capabilities\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "print(f\"Major Version: {major_version}, Minor Version: {minor_version}\")\n",
    "\n",
    "#Configuration\n",
    "max_seq_length = 2048  #Token sequence length\n",
    "dtype = None  #\"None\" for automatic detection\n",
    "\n",
    "#Model configurations\n",
    "model_name = \"unsloth/Qwen2-1.5B-Instruct-bnb-4bit\"\n",
    "load_in_4bit = True  # Load the model in 4 bits\n",
    "\n",
    "#Load the model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name, \n",
    "    load_in_4bit=load_in_4bit,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the new parameter: torch.Size([2, 1536])\n",
      "Original shape of lm_head.weight: torch.Size([151936, 1536])\n"
     ]
    }
   ],
   "source": [
    "#Obtain token IDs for \"Yes\" and \"No\"\n",
    "yes_token_id = tokenizer.encode(\"Sim\", add_special_tokens=False)[0]  #Yes\n",
    "no_token_id = tokenizer.encode(\"N√£o\", add_special_tokens=False)[0]  #No\n",
    "\n",
    "#Create a tensor with the weights of the lm_head for the tokens \"Yes\" and \"No\"\n",
    "par = torch.nn.Parameter(torch.vstack([\n",
    "    model.lm_head.weight[no_token_id, :],  #Representation for class 0 (No)\n",
    "    model.lm_head.weight[yes_token_id, :]  #Representation for class 1 (Yes)\n",
    "]))\n",
    "\n",
    "#Check the shapes before and after the replacement\n",
    "print(f\"Shape of the new parameter: {par.shape}\")\n",
    "print(f\"Original shape of lm_head.weight: {model.lm_head.weight.shape}\")\n",
    "\n",
    "#Replace the weights of the lm_head with the new parameter\n",
    "model.lm_head.weight = par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaopedro/joaopedro/llm/lib/python3.10/site-packages/unsloth/models/_utils.py:755: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n",
      "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n",
      "trainable parameters: 18467840\n"
     ]
    }
   ],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"lm_head\",\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "Dividing the dataset into training, validation and testing using stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the data\n",
    "data = pd.read_csv(\"/home/joaopedro/joaopedro/llm/Mestrado/Datasets/Boamente_Treino_Atualizado.csv\")\n",
    "\n",
    "#Defining the sizes for the training and validation sets\n",
    "train_size = 3030  #Number of samples in the training set\n",
    "val_size = 758  #Number of samples in the validation set\n",
    "\n",
    "#Sampling the data\n",
    "data_sample = data.sample(n=train_size + val_size, random_state=24)\n",
    "\n",
    "#Splitting the data into training and validation sets with stratification\n",
    "train_df, val_df = train_test_split(\n",
    "    data_sample,\n",
    "    test_size=val_size / (train_size + val_size),\n",
    "    random_state=24,\n",
    "    stratify=data_sample['label']\n",
    ")\n",
    "\n",
    "#Function to add text labels above the bars\n",
    "def add_labels(ax, df):\n",
    "    for i, value in enumerate(df):\n",
    "        ax.text(i, value + 30, str(value), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "#Specific colors\n",
    "colors = ['#1f77b4', '#ff7f0e']  #Soft blue and orange\n",
    "\n",
    "#Create bar charts\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "#Data configuration for each chart\n",
    "datasets = [('', data), ('', train_df), ('', val_df)]\n",
    "labels = ['(a)', '(b)', '(c)']\n",
    "\n",
    "for ax, label, (title, df) in zip(axs, labels, datasets):\n",
    "    counts = df['label'].value_counts().sort_index()\n",
    "    counts.plot(kind='bar', ax=ax, color=colors, width=0.8)\n",
    "    ax.set_title(f'{title}', fontsize=14)\n",
    "    ax.set_xticklabels(['Negative', 'Positive'], rotation=0)\n",
    "    add_labels(ax, counts)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Count' if label == '(a)' else '')\n",
    "\n",
    "#Add labels (a), (b), (c) below each subplot\n",
    "for ax, label in zip(axs, labels):\n",
    "    ax.text(0.5, -0.2, label, transform=ax.transAxes, ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "#Define legend labels with colors\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color='#1f77b4', label='Negative'),  #Soft blue\n",
    "    mpatches.Patch(color='#ff7f0e', label='Positive')   #Soft orange\n",
    "]\n",
    "\n",
    "#Add the legend inside subplot (c), centered\n",
    "axs[2].legend(handles=legend_patches, loc='best', fontsize=12, frameon=False)\n",
    "\n",
    "#Adjust layout and show the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show validation set data and plot the token number histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "3187                                       vou me matar      1\n",
      "2779  meu namorado da academia veio todo de preto a√≠...      0\n",
      "3712                                  Ai √© pra me matar      0\n",
      "1399              maneiras de me matar google pesquisar      1\n",
      "3311  essa lua em escorpi√£o vai me matar, tenho abso...      0\n",
      "...                                                 ...    ...\n",
      "1582                       Eu vou cometer suic√≠dio!!!!!      1\n",
      "275   escrevendo minha carta de suic√≠dio chorando e ...      1\n",
      "79    √â q a gente n quer s√≥ comida mas tamo cansado ...      0\n",
      "3501  me matar pelo porr da empresa p esperarem at√© ...      0\n",
      "3049                                       Vou me matar      1\n",
      "\n",
      "[758 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3030\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "\n",
    "#Print the validation set\n",
    "print(val_df)\n",
    "\n",
    "#Count the tokens for each text in the training set\n",
    "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
    "\n",
    "#Plot the histogram of token counts\n",
    "plt.hist(token_counts, bins=30, edgecolor='black')\n",
    "plt.title(\"Distribution of Token Counts per Text\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "#Convert the DataFrame into a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Aqui est√° uma senten√ßa.\n",
    "{}\n",
    "\n",
    "Se a senten√ßa cont√©m idea√ß√£o suicida, responda \"Sim\". Caso contr√°rio, responda \"N√£o\".\n",
    "\n",
    "SOLU√á√ÉO\n",
    "A resposta correta √©: \"{}\"\"\"\n",
    "\n",
    "positivelabel = \"Sim\" #Yes\n",
    "negativelabel = \"N√£o\" #No\n",
    "\n",
    "# This is a sentence.\n",
    "# {}\n",
    "#\n",
    "# If the sentence contains suicidal ideation, respond with \"Yes\". Otherwise, respond with \"No\".\n",
    "#\n",
    "# SOLUTION\n",
    "# The correct answer is: \"{}\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(dataset_):\n",
    "    #Check the type of text\n",
    "    if isinstance(dataset_['text'], str):\n",
    "        #Specific adjustments for different models (Qwen, Llama, etc.)\n",
    "        if 'qwen' in model_name.lower():\n",
    "            return [\"\"] * 100  #Specific case for the Qwen model\n",
    "        elif 'llama' in model_name.lower():\n",
    "            return \" \"  #Specific case for the Llama model\n",
    "        else:\n",
    "            return \" \"  #General case\n",
    "\n",
    "    texts = []\n",
    "    #Iterate through all examples in the dataset\n",
    "    for i in range(len(dataset_['text'])):\n",
    "        t = dataset_['text'][i]  #Extract the tweet text\n",
    "        label = positivelabel if dataset_['label'][i] == 1 else negativelabel  #Determine the label\n",
    "        text = prompt.format(t, label)  #Format the prompt with text and label\n",
    "        texts.append(text)  #Add to the list of texts\n",
    "\n",
    "    return texts  #Return the list of formatted prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from typing import List, Union, Dict, Any\n",
    "\n",
    "#Custom collator to modify labels and train only on the last token\n",
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, *args, mlm: bool = False, ignore_index: int = -100, **kwargs):\n",
    "        #Initialize the parent class with the provided arguments\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index  # Define the ignore_index value, which will be used to mask tokens\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        #Call the parent class function to prepare the batch of examples\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        #Modify the batch to train only on the last token\n",
    "        for i in range(len(examples)):\n",
    "            #Find the index of the last non-ignored token\n",
    "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            #Ignore all tokens except for the last one\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            #Assign 1 or 0 to the last token, depending on whether it is the target token\n",
    "            batch[\"labels\"][i, last_token_idx] = 1 if batch[\"labels\"][i, last_token_idx] == yes_token_id else 0\n",
    "\n",
    "        return batch\n",
    "\n",
    "#Instantiate the collator\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Iniciando Fold 1/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b39652d4ee4e75bd29fb65ffb14e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11cbf1349c84a48b65f4f9bb2d16c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c08e32eb13a43bd8cbefd4c93bca591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f911bcacf54ecab8e9d0fe48bff954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2f4fab0c3346f1a1b3e002f0e5a1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d11eb4b8f6946a09f1866dad63a514b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256bb68e29764f2b909f90619763dc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e09dc6079394c75b2f66bb0f293b3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,424 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 76\n",
      " \"-____-\"     Number of trainable parameters = 18,467,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.925900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resultados do Fold 1: {'eval_loss': 0.9205198287963867, 'eval_runtime': 2.4308, 'eval_samples_per_second': 249.3, 'eval_steps_per_second': 31.265}\n",
      "\n",
      "üöÄ Iniciando Fold 2/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9f30eae0804c058b11fc68de6b831c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b016b8cc7974526833184305632987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fdb4ca9f0049ffa2673aec01bc2fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a605a4e7687b43e383cfb5a6c05ed5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136cd145e7df45aea94613e358a29f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45971a36fb50426cac666d4b169197f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a1f2aae6034c64824271fe9aaf1eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fa6c8a6bc44794b330ef4c28f25525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,424 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 76\n",
      " \"-____-\"     Number of trainable parameters = 18,467,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.102900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.465700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resultados do Fold 2: {'eval_loss': 1.0242451429367065, 'eval_runtime': 2.1254, 'eval_samples_per_second': 285.125, 'eval_steps_per_second': 35.758}\n",
      "\n",
      "üöÄ Iniciando Fold 3/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5a131bebfa4fb78b5eae704059705e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397431061d804e649530f28574e8844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b508cb8f814f42e9bc52c1415048e5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6ea9e81ff64eee824953c91d370eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a59e3d870a448bdbe27d819ee822e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f1c64e50914712885ad92dec7e87c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4b7a4b050045a7be73d682bd436482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f163e4312bed4a35945dd1876fea7d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,424 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 76\n",
      " \"-____-\"     Number of trainable parameters = 18,467,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resultados do Fold 3: {'eval_loss': 0.9405980706214905, 'eval_runtime': 2.1125, 'eval_samples_per_second': 286.867, 'eval_steps_per_second': 35.977}\n",
      "\n",
      "üöÄ Iniciando Fold 4/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60d3086f61f4acea9fae6265fb4bbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2de40583354eb1866756065d44aa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f695fe3464431f8739a0aa5a0d7ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffa51167a404b1982c903681ec80381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9bf5a0a9b64667bdbda49d64f9e4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f415bf45d844427099b1903f91e2fa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef97548d37841afbaa053f982ae60e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75f4ace4bc24c9cb2b89d9594a1cafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,424 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 76\n",
      " \"-____-\"     Number of trainable parameters = 18,467,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>-0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resultados do Fold 4: {'eval_loss': 0.8850820660591125, 'eval_runtime': 2.1064, 'eval_samples_per_second': 287.7, 'eval_steps_per_second': 36.081}\n",
      "\n",
      "üöÄ Iniciando Fold 5/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab627b1d9afb47b7a9ee890f5c371430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32109790f1043968de1a30b8ba475e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75056d3f0cc14ab191e0d4a7ea417a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3d805e09b04a8487aa434810f5a7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea779e78024226bc24011f2981f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d80d90ec5240db82da41874b73bc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec203e526c349e4ba94bb3f54ba330f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5c012e6ad84d71a217c9477081d930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,424 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 76\n",
      " \"-____-\"     Number of trainable parameters = 18,467,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>-0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>-0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Resultados do Fold 5: {'eval_loss': 0.9905345439910889, 'eval_runtime': 2.106, 'eval_samples_per_second': 287.755, 'eval_steps_per_second': 36.088}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datasets import Dataset\n",
    "\n",
    "#N√∫mero de folds para valida√ß√£o cruzada\n",
    "num_folds = 5\n",
    "\n",
    "#Converter o dataset do pandas para o formato do Hugging Face\n",
    "full_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "\n",
    "#Criar os √≠ndices dos folds para valida√ß√£o cruzada\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=3407)\n",
    "\n",
    "#Armazenar m√©tricas de cada fold\n",
    "all_results = []\n",
    "\n",
    "#Iterar sobre os folds\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(full_dataset[\"text\"], full_dataset[\"label\"])):\n",
    "    print(f\"\\nüöÄ Iniciando Fold {fold+1}/{num_folds}...\")\n",
    "\n",
    "    #Criar os datasets para treino e valida√ß√£o\n",
    "    train_dataset = full_dataset.select(train_idx)\n",
    "    val_dataset = full_dataset.select(val_idx)\n",
    "\n",
    "    #Definir os argumentos do treinamento\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"outputs/fold_{fold+1}\",\n",
    "        num_train_epochs=1,\n",
    "        report_to=\"none\",\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    #Criar o Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    #Treinar o modelo\n",
    "    trainer.train()\n",
    "\n",
    "    #Avaliar o modelo no conjunto de valida√ß√£o\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"üìä Resultados do Fold {fold+1}: {metrics}\")\n",
    "\n",
    "    #Armazenar as m√©tricas do fold\n",
    "    all_results.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training 80/20 only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  \n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs=1,\n",
    "        report_to=\"none\",  \n",
    "        group_by_length=True,\n",
    "    ),\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536, padding_idx=151643)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1536, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1536, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**\n",
    "\n",
    "Metrics: Accuracy, Precision, Recall, F1-Score, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:02<00:00, 32.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.2689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[523  15]\n",
      " [ 23 197]]\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.9499\n",
      "Precision: 0.9292\n",
      "Recall: 0.8955\n",
      "F1-Score: 0.9120\n",
      "AUC: 0.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "#Define output directory\n",
    "output_dir = \"/home/joaopedro/joaopedro/llm/Mestrado/curvaROC/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Load validation data\n",
    "val_texts = val_df['text'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "#Define storage\n",
    "all_probabilities = []\n",
    "all_labels = []\n",
    "\n",
    "#Step 1: Tokenization and sorting by token length\n",
    "tokenized_inputs = []\n",
    "for text, label in zip(val_texts, val_labels):\n",
    "    test_str = prompt.format(text, \"\")\n",
    "    tokenized_input = tokenizer(test_str, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    tokenized_inputs.append((tokenized_input, test_str, label))\n",
    "\n",
    "#Sort by tokenized length\n",
    "tokenized_inputs.sort(key=lambda x: x[0]['input_ids'].shape[1])\n",
    "\n",
    "#Step 2: Group by tokenized length\n",
    "grouped_inputs = defaultdict(list)\n",
    "for tokenized_input, test_str, label in tokenized_inputs:\n",
    "    length = tokenized_input['input_ids'].shape[1]\n",
    "    grouped_inputs[length].append((tokenized_input, test_str, label))\n",
    "\n",
    "#Step 3: Process batches\n",
    "batch_size = 64\n",
    "\n",
    "for length, group in tqdm(grouped_inputs.items()):\n",
    "    for i in range(0, len(group), batch_size):\n",
    "        batch = group[i:i + batch_size]\n",
    "        batch_inputs = [item[0] for item in batch]\n",
    "        batch_labels = [item[2] for item in batch]\n",
    "\n",
    "        #Concatenate batch inputs\n",
    "        input_ids = torch.cat([item['input_ids'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
    "        attention_mask = torch.cat([item['attention_mask'] for item in batch_inputs], dim=0).to(\"cuda\")\n",
    "\n",
    "        #Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #Extract logits for classification\n",
    "        logits = outputs.logits[:, -1, :2]\n",
    "        probabilities = F.softmax(logits, dim=-1)[:, 1].cpu().numpy()  #Probability of class 1\n",
    "\n",
    "        #Store results\n",
    "        all_probabilities.extend(probabilities)\n",
    "        all_labels.extend(batch_labels)\n",
    "\n",
    "#Step 4: Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#Save ROC data\n",
    "np.savetxt(os.path.join(output_dir, \"fpr_Qwen2.5_1.5B.txt\"), fpr)\n",
    "np.savetxt(os.path.join(output_dir, \"tpr_Qwen2.5_1.5B.txt\"), tpr)\n",
    "np.savetxt(os.path.join(output_dir, \"thresholds_Qwen2.5_1.5B.txt\"), thresholds)\n",
    "with open(os.path.join(output_dir, \"auc_Qwen2.5_1.5B.txt\"), \"w\") as f:\n",
    "    f.write(f\"AUC: {roc_auc:.4f}\\n\")\n",
    "\n",
    "#Step 5: Find optimal threshold\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "#Step 6: Convert probabilities to binary predictions\n",
    "all_outputs = (all_probabilities >= optimal_threshold).astype(int)\n",
    "\n",
    "#Step 7: Compute evaluation metrics\n",
    "cm = confusion_matrix(all_labels, all_outputs)\n",
    "accuracy = accuracy_score(all_labels, all_outputs)\n",
    "precision = precision_score(all_labels, all_outputs)\n",
    "recall = recall_score(all_labels, all_outputs)\n",
    "f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "#Step 8: Save all metrics + confusion matrix to a single file\n",
    "metrics_file = os.path.join(output_dir, \"metrics_Qwen2.5_1.5B.txt\")\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    f.write(\"===== Model Evaluation Metrics =====\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {f1:.4f}\\n\")\n",
    "    f.write(f\"AUC: {roc_auc:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n===== Confusion Matrix =====\\n\")\n",
    "    np.savetxt(f, cm, fmt=\"%d\")\n",
    "\n",
    "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "\n",
    "#Step 9: Save confusion matrix separately\n",
    "np.savetxt(os.path.join(output_dir, \"confusion_matrix_Qwen2.5_1.5B.txt\"), cm, fmt='%d')\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve and Confusion Matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Define the directory where the generated files are stored\n",
    "output_dir = \"/home/joaopedro/joaopedro/llm/Mestrado/curvaROC/\"\n",
    "\n",
    "#File names\n",
    "metrics_file = os.path.join(output_dir, \"metrics_Qwen2.5_1.5B.txt\")\n",
    "confusion_matrix_file = os.path.join(output_dir, \"confusion_matrix_Qwen2.5_1.5B.txt\")\n",
    "fpr_file = os.path.join(output_dir, \"fpr_Qwen2.5_1.5B.txt\")\n",
    "tpr_file = os.path.join(output_dir, \"tpr_Qwen2.5_1.5B.txt\")\n",
    "auc_file = os.path.join(output_dir, \"auc_Qwen2.5_1.5B.txt\")\n",
    "\n",
    "#Load metrics\n",
    "metrics = {}\n",
    "with open(metrics_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            key, value = line.strip().split(\": \")\n",
    "            try:\n",
    "                metrics[key.lower()] = float(value)\n",
    "            except ValueError:\n",
    "                pass  #Ignore non-numeric values\n",
    "\n",
    "#Extract metric values\n",
    "accuracy = metrics.get(\"accuracy\", 0.0)\n",
    "precision = metrics.get(\"precision\", 0.0)\n",
    "recall = metrics.get(\"recall\", 0.0)\n",
    "f1 = metrics.get(\"f1-score\", 0.0)\n",
    "auc_score = metrics.get(\"auc\", 0.0)\n",
    "\n",
    "#Load confusion matrix\n",
    "cm = np.loadtxt(confusion_matrix_file, dtype=int)\n",
    "\n",
    "#Load ROC curve data\n",
    "fpr = np.loadtxt(fpr_file)\n",
    "tpr = np.loadtxt(tpr_file)\n",
    "\n",
    "#Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "#Plot ROC Curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(output_dir, \"roc_curve.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved at: /home/joaopedro/joaopedro/llm/Mestrado/Models/Qwen2.5_1.5B_finetuned\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"/home/joaopedro/joaopedro/llm/Mestrado/Models/Qwen2.5_1.5B_finetuned\" #Change the file name to the template you are running\n",
    "\n",
    "#Save model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved at: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapter and tokenizer saved at: /home/joaopedro/joaopedro/llm/Mestrado/Models/Qwen2.5_1.5B_Peft_finetuned\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_save_path = \"/home/joaopedro/joaopedro/llm/Mestrado/Models/Qwen2.5_1.5B_Peft_finetuned\"\n",
    "\n",
    "#Save only the LoRA adapter\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ LoRA adapter and tokenizer saved at: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
